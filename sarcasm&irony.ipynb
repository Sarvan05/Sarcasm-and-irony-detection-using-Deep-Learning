{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYx9eZXzd4Gv",
        "outputId": "a9c761ee-38fc-4a78-f752-f07023e90d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "import joblib\n",
        "import torch\n",
        "import string\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CyDq9DRYeVkM"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCOc0JrVWvjO",
        "outputId": "1d8e03ec-d10a-4113-8407-a52a464f55cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (40218, 2)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv('train.csv')\n",
        "print(\"Dataset shape:\", dataset.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "55NZq3zrikks",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "3d11186f-3a98-4e1b-c82a-715be35202ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def remove_URL(text):\\n    text=str(text)\\n    url = re.compile(r\\'https?://\\\\S+|www\\\\.\\\\S+\\')\\n    return url.sub(r\\'\\', text)\\n\\n\\ndef remove_emoji(text):\\n    emoji_pattern = re.compile(\\n        \\'[\\'\\n        u\\'ğŸ˜€-ğŸ™\\'\\n        u\\'ğŸŒ€-ğŸ—¿\\'\\n        u\\'ğŸš€-\\U0001f6ff\\'\\n        u\\'\\U0001f1e0-ğŸ‡¿\\'\\n        u\\'âœ‚-â°\\'\\n        u\\'â“‚-ğŸ‰‘\\'\\n        \\']+\\',\\n        flags=re.UNICODE)\\n    return emoji_pattern.sub(r\\'\\', text)\\n\\ndef remove_mentions(text):\\n    ment = re.compile(r\"(@[A-Za-z0-9]+)\")\\n    return ment.sub(r\\'\\', text)\\n\\n\\ndef remove_html(text):\\n    html = re.compile(r\\'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\\')\\n    return re.sub(html, \\'\\', text)\\n\\n\\ndef remove_punct(text):\\n    table = str.maketrans(\\'\\', \\'\\', string.punctuation)\\n    return text.translate(table)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "'''def remove_URL(text):\n",
        "    text=str(text)\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        '['\n",
        "        u'\\U0001F600-\\U0001F64F'\n",
        "        u'\\U0001F300-\\U0001F5FF'\n",
        "        u'\\U0001F680-\\U0001F6FF'\n",
        "        u'\\U0001F1E0-\\U0001F1FF'\n",
        "        u'\\U00002702-\\U000027B0'\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+',\n",
        "        flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def remove_mentions(text):\n",
        "    ment = re.compile(r\"(@[A-Za-z0-9]+)\")\n",
        "    return ment.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_html(text):\n",
        "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    return re.sub(html, '', text)\n",
        "\n",
        "\n",
        "def remove_punct(text):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(table)'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import urllib.parse\n",
        "def remove_URL(text):\n",
        "    text = str(text)\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', text)\n",
        "\n",
        "def retain_emoji(text):\n",
        "    text = emoji.demojize(text, delimiters=(\"\", \" \"))\n",
        "    return text\n",
        "\n",
        "def remove_mentions(text):\n",
        "    ment = re.compile(r\"(@[A-Za-z0-9_]+)\")\n",
        "    return ment.sub(r'', text)\n",
        "\n",
        "def remove_html(text):\n",
        "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    return re.sub(html, '', text)\n",
        "\n",
        "def remove_punct(text):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = remove_URL(text)\n",
        "    text = retain_emoji(text)\n",
        "    text = remove_mentions(text)\n",
        "    text = remove_html(text)\n",
        "    text = remove_punct(text)\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "7IYfKlzPjS13"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "aiII4z-Hi-C7"
      },
      "outputs": [],
      "source": [
        "train_data['clean_text'] = train_data['tweets'].apply(lambda x: preprocess_text(x))\n",
        "train_data['clean_text'] = train_data['clean_text'].apply(lambda x: x.lower())\n",
        "cleaned = train_data['clean_text'].tolist()\n",
        "\n",
        "for i,text in enumerate(cleaned):\n",
        "    splits = text.split()\n",
        "    splits = [word for word in splits if word not in set(nltk.corpus.stopwords.words('english'))]\n",
        "    cleaned[i]=' '.join(splits)\n",
        "\n",
        "train_data['clean_text']=cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "bvY4phgSkbcL"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_data['class'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7oYeijpoz-p",
        "outputId": "4b43a534-58d1-4863-8b90-0ab30eedd9b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Label: irony, Encoded Label: 0\n",
            "Original Label: sarcasm, Encoded Label: 1\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_data['class'].values)\n",
        "\n",
        "\n",
        "class_labels = train_data['class'].unique()\n",
        "\n",
        "\n",
        "for label, encoded_label in zip(class_labels, label_encoder.transform(class_labels)):\n",
        "    print(f\"Original Label: {label}, Encoded Label: {encoded_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV7pDQHaqS6Y",
        "outputId": "8bb8af7b-d2da-40fc-e927-1c594935a218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training samples: 32174\n",
            "Validation samples: 8044\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_data['tweets'].tolist(),\n",
        "    train_data['class'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining samples: {len(train_texts)}\")\n",
        "print(f\"Validation samples: {len(val_texts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4dfJbZm2q6_B"
      },
      "outputs": [],
      "source": [
        "def scores(y_test, y_predicted):\n",
        "\n",
        "  accuracy = accuracy_score(y_test, y_predicted)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "\n",
        "  precision = precision_score(y_test, y_predicted, average='weighted')\n",
        "  print('Precision: %f' % precision)\n",
        "\n",
        "  recall = recall_score(y_test, y_predicted, average='weighted')\n",
        "  print('Recall: %f' % recall)\n",
        "\n",
        "  f1 = f1_score(y_test, y_predicted, average='weighted')\n",
        "  print('F1 score: %f' % f1)\n",
        "  return precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHv4Sl3mtqWG",
        "outputId": "5d541ed7-475e-492f-b248-88101f9591db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('train.csv')\n",
        "\n",
        "x = train_data['tweets'].tolist()\n",
        "\n",
        "stopWords = set(stopwords.words('english'))\n",
        "\n",
        "filtered_tweets = []\n",
        "for tweet in x:\n",
        "    words = tweet.split()\n",
        "    words_filtered = [word for word in words if word.lower() not in stopWords]\n",
        "    filtered_tweets.append(\" \".join(words_filtered))\n",
        "\n",
        "vectorizer = CountVectorizer(lowercase=True)\n",
        "features = vectorizer.fit_transform(filtered_tweets)\n",
        "\n",
        "features_nd = features.toarray()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(train_data['class'].values)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(features_nd, y, train_size=0.80, random_state=1234)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "log_model = LogisticRegression(max_iter=1000)\n",
        "log_model.fit(x_train, y_train)\n",
        "\n",
        "y_pred = log_model.predict(x_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WH99o89IRAfF",
        "outputId": "cdded964-efe3-41e5-8ab0-8e16ea606cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset columns: Index(['tweets', 'class'], dtype='object')\n",
            "                                              tweets  class\n",
            "0  Fav moment in Sepp Blatter vid (0:20): \"We hav...  irony\n",
            "1  Just found this while walking my human.... #ir...  irony\n",
            "2  'Disrespected the wife of Prophet' - pseudo li...  irony\n",
            "3  Do you know that super yeay satisfying feeling...  irony\n",
            "4  If you're going to call someone ignorant and s...  irony\n",
            "Dataset shape after cleaning: (40218, 2)\n",
            "Shape of tokenized and padded input features: (40218, 35)\n",
            "Shape of one-hot encoded labels: (40218, 2)\n",
            "X_train shape: (32174, 35), Y_train shape: (32174, 2)\n",
            "X_test shape: (8044, 35), Y_test shape: (8044, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m128\u001b[0m)             â”‚       \u001b[38;5;34m3,840,000\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m512\u001b[0m)             â”‚         \u001b[38;5;34m788,480\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m512\u001b[0m)             â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚         \u001b[38;5;34m656,384\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚          \u001b[38;5;34m32,896\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   â”‚             \u001b[38;5;34m258\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">788,480</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">656,384</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,318,018\u001b[0m (20.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,318,018</span> (20.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,318,018\u001b[0m (20.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,318,018</span> (20.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m503/503\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 720ms/step - accuracy: 0.8354 - loss: 0.3017\n",
            "Epoch 2/10\n",
            "\u001b[1m503/503\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 718ms/step - accuracy: 0.9992 - loss: 0.0035\n",
            "Epoch 3/10\n",
            "\u001b[1m503/503\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 720ms/step - accuracy: 0.9990 - loss: 0.0038\n",
            "Epoch 4/10\n",
            "\u001b[1m503/503\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 714ms/step - accuracy: 0.9999 - loss: 7.8646e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m503/503\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 713ms/step - accuracy: 0.9997 - loss: 0.0015\n",
            "Epoch 6/10\n",
            "\u001b[1m503/503\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 715ms/step - accuracy: 0.9999 - loss: 4.4719e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m503/503\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 708ms/step - accuracy: 0.9998 - loss: 0.0012\n",
            "Epoch 8/10\n",
            "\u001b[1m503/503\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 728ms/step - accuracy: 0.9997 - loss: 0.0015\n",
            "Epoch 9/10\n",
            "\u001b[1m503/503\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 724ms/step - accuracy: 0.9999 - loss: 3.8231e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m503/503\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 725ms/step - accuracy: 0.9997 - loss: 7.1929e-04\n",
            "\u001b[1m126/126\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 201ms/step - accuracy: 0.9986 - loss: 0.0157\n",
            "Test loss, test accuracy: [0.010071956552565098, 0.9987568259239197]\n",
            "\u001b[1m252/252\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 128ms/step\n",
            "Accuracy: 0.9987568373943312\n",
            "Precision: 0.9987569836635299\n",
            "Recall: 0.9987568373943312\n",
            "F1 Score: 0.9987568487063905\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "dataset = pd.read_csv(\"/train.csv\", sep=',')\n",
        "\n",
        "\n",
        "print(\"Dataset columns:\", dataset.columns)\n",
        "print(dataset.head())\n",
        "\n",
        "\n",
        "dataset = dataset.dropna(subset=['tweets', 'class'])\n",
        "\n",
        "\n",
        "print(f\"Dataset shape after cleaning: {dataset.shape}\")\n",
        "\n",
        "\n",
        "text_column_name = 'tweets'\n",
        "label_column_name = 'class'\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=30000, split=' ')\n",
        "tokenizer.fit_on_texts(dataset[text_column_name].values)\n",
        "\n",
        "\n",
        "X = tokenizer.texts_to_sequences(dataset[text_column_name].values)\n",
        "\n",
        "\n",
        "X = pad_sequences(X, padding='post', maxlen=35)\n",
        "\n",
        "\n",
        "print(f\"Shape of tokenized and padded input features: {X.shape}\")\n",
        "\n",
        "\n",
        "Y = pd.get_dummies(dataset[label_column_name]).values\n",
        "\n",
        "\n",
        "print(f\"Shape of one-hot encoded labels: {Y.shape}\")\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, random_state=42)\n",
        "\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, Y_test shape: {Y_test.shape}\")\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.shuffle(buffer_size=10000).batch(64)\n",
        "test_dataset = test_dataset.batch(64)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Embedding(input_dim=30000, output_dim=128, input_length=35),\n",
        "\n",
        "\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
        "\n",
        "\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
        "\n",
        "\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "\n",
        "\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.build(input_shape=(None, 35))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = model.fit(train_dataset, epochs=10)\n",
        "\n",
        "\n",
        "results = model.evaluate(test_dataset)\n",
        "print('Test loss, test accuracy:', results)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "y_test_classes = Y_test.argmax(axis=1)\n",
        "\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(y_test_classes, y_pred_classes)}')\n",
        "print(f'Precision: {precision_score(y_test_classes, y_pred_classes, average=\"weighted\")}')\n",
        "print(f'Recall: {recall_score(y_test_classes, y_pred_classes, average=\"weighted\")}')\n",
        "print(f'F1 Score: {f1_score(y_test_classes, y_pred_classes, average=\"weighted\")}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"I love getting mail saying that my medical &amp; personal information has been compromised. #sarcasm\",\n",
        "    \"This has been the worst experience of my life.\",\n",
        "    \"Oh great, another Monday morning. Just what I needed.\",\n",
        "    \"The fire station burned down last night.\",\n",
        "    \"I went to the store to buy some groceries.\"\n",
        "]\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "\n",
        "predictions = model.predict(padded_sequences)\n",
        "\n",
        "\n",
        "class_labels = ['irony', 'sarcasm']\n",
        "\n",
        "\n",
        "print(\"Prediction Results:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    predicted_class_index = np.argmax(predictions[i])\n",
        "    predicted_class = class_labels[predicted_class_index]\n",
        "    print(f\"Input Sentence: {sentence}\")\n",
        "    print(f\"Prediction: {predicted_class} (Probabilities: {predictions[i]})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbk08XbLgviy",
        "outputId": "af2a4687-46f5-4789-c903-1ecbaadf8f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723ms/step\n",
            "Prediction Results:\n",
            "Input Sentence: I love getting mail saying that my medical &amp; personal information has been compromised. #sarcasm\n",
            "Prediction: irony (Probabilities: [9.9968445e-01 3.1557016e-04])\n",
            "Input Sentence: This has been the worst experience of my life.\n",
            "Prediction: irony (Probabilities: [1.0000000e+00 1.2935479e-08])\n",
            "Input Sentence: Oh great, another Monday morning. Just what I needed.\n",
            "Prediction: sarcasm (Probabilities: [5.2503660e-06 9.9999475e-01])\n",
            "Input Sentence: The fire station burned down last night.\n",
            "Prediction: irony (Probabilities: [1.0000000e+00 1.5658651e-08])\n",
            "Input Sentence: I went to the store to buy some groceries.\n",
            "Prediction: irony (Probabilities: [9.9997509e-01 2.4897232e-05])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7cJAd6jeqcy",
        "outputId": "062ff5e2-ca0d-42b3-acca-ad7fa7e95b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved at: /content/saved_models/sarcasm_and_irony_model.h5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "save_dir = \"/content/saved_models\"\n",
        "\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "\n",
        "model_path = os.path.join(save_dir, \"sarcasm_and_irony_model.h5\")\n",
        "\n",
        "\n",
        "model.save(model_path)\n",
        "\n",
        "print(f\"Model saved at: {model_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}